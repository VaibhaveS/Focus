{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dfaf798a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package nps_chat to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package nps_chat is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#! pip install PyAudio\n",
    "#! pip install SpeechRecognition\n",
    "import pandas as pd\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "#!pip install azure-cognitiveservices-speech\n",
    "#!pip install google-api-python-client \n",
    "#!pip install --upgrade google-cloud-speech\n",
    "#!pip install anvil-uplink\n",
    "#!pip install --upgrade google-api-python-client\n",
    "#!pip install --upgrade google-cloud-speech\n",
    "\n",
    "from azure.cognitiveservices.speech import AudioDataStream, SpeechConfig, SpeechSynthesizer, SpeechSynthesisOutputFormat\n",
    "from azure.cognitiveservices.speech.audio import AudioOutputConfig\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import anvil.server\n",
    "anvil.server.connect(\"I6B7FZ3SMLFENTUGQOYGCTMC-NUKYRRNENRSFFQ4A\")\n",
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"C:/Users/HP/Downloads/core-phoenix-341914-55f3e91fad73.json\"\n",
    "import re\n",
    "import nltk.corpus\n",
    "from nltk.corpus import nps_chat\n",
    "import pandas as pd\n",
    "from sklearn.svm import LinearSVC\n",
    "import nltk\n",
    "import math\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('nps_chat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a2ae1d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "import collections\n",
    "from nltk.metrics import precision\n",
    "from nltk.metrics import recall\n",
    "from nltk.metrics import ConfusionMatrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "class isQuestion():\n",
    "    def __init__(self):\n",
    "        posts = nltk.corpus.nps_chat.xml_posts()\n",
    "        features = self.__get_feature_set(posts)\n",
    "        new_features=[]\n",
    "        for j in features: \n",
    "            label=j[1]\n",
    "            if(label!='whQuestion' and label!='ynQuestion'):\n",
    "                label='NotQuestion'\n",
    "            new_features.append((j[0],label))\n",
    "        features=new_features\n",
    "        #self.classifier = self.naiveBayes(features)\n",
    "        #self.classifier = self.DecisionTree(features)\n",
    "        #self.classifier = self.MaxentClassifier(features)\n",
    "        self.classifier = self.SVM(features)\n",
    "        #self.classifier = self.KNN(features)\n",
    "    def __get_feature_set(self, posts):\n",
    "        feature = []\n",
    "        for post in posts:\n",
    "            post_text = post.text            \n",
    "            features = {}\n",
    "            words = nltk.word_tokenize(post_text)\n",
    "            for word in words:\n",
    "                features['contains({})'.format(word.lower())] = True\n",
    "            feature.append((features, post.get('class')))\n",
    "        return feature\n",
    "    def naiveBayes(self, feature_set):\n",
    "        training_size = int(len(feature_set) * 0.1)\n",
    "        train_set, test_set = feature_set[training_size:], feature_set[:training_size]\n",
    "        classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "        print(\"accuracy = \",\"{0:.5f}\".format(nltk.classify.accuracy(classifier, test_set)))\n",
    "        refsets = collections.defaultdict(set)\n",
    "        testsets = collections.defaultdict(set)\n",
    "        for i, (feats, label) in enumerate(test_set):\n",
    "            refsets[label].add(i)\n",
    "            observed = classifier.classify(feats)\n",
    "            testsets[observed].add(i)\n",
    "        print(\"\\t\\tWH-Question\\t\\tYN-Question\\t\\tNotQuestion\\n\")\n",
    "        print( 'Precision:\\t', \"{0:.5f}\".format(precision(refsets['whQuestion'], testsets['whQuestion'])), \"\\t\\t\", \"{0:.5f}\".format(precision(refsets['ynQuestion'], testsets['ynQuestion'])), \"\\t\\t\", \"{0:.5f}\".format(precision(refsets['NotQuestion'], testsets['NotQuestion'])))\n",
    "        print( 'Recall:   \\t', \"{0:.5f}\".format(recall(refsets['whQuestion'], testsets['whQuestion'])), \"\\t\\t\", \"{0:.5f}\".format(recall(refsets['ynQuestion'], testsets['ynQuestion'])), \"\\t\\t\", \"{0:.5f}\".format(recall(refsets['NotQuestion'], testsets['NotQuestion'])))\n",
    "        return classifier\n",
    "    def MaxentClassifier(self, feature_set):\n",
    "        training_size = int(len(feature_set) * 0.1)\n",
    "        train_set, test_set = feature_set[training_size:], feature_set[:training_size]\n",
    "        classifier = nltk.MaxentClassifier.train(train_set,max_iter=15)\n",
    "        print(\"accuracy = \",\"{0:.5f}\".format(nltk.classify.accuracy(classifier, test_set)))\n",
    "        refsets = collections.defaultdict(set)\n",
    "        testsets = collections.defaultdict(set)\n",
    "\n",
    "        for i, (feats, label) in enumerate(test_set):\n",
    "            refsets[label].add(i)\n",
    "            observed = classifier.classify(feats)\n",
    "            testsets[observed].add(i)\n",
    "        print(\"\\t\\tWH-Question\\t\\tYN-Question\\t\\tNotQuestion\\n\")\n",
    "        print( 'Precision:\\t', \"{0:.5f}\".format(precision(refsets['whQuestion'], testsets['whQuestion'])), \"\\t\\t\", \"{0:.5f}\".format(precision(refsets['ynQuestion'], testsets['ynQuestion'])), \"\\t\\t\", \"{0:.5f}\".format(precision(refsets['NotQuestion'], testsets['NotQuestion'])))\n",
    "        print( 'Recall:   \\t', \"{0:.5f}\".format(recall(refsets['whQuestion'], testsets['whQuestion'])), \"\\t\\t\", \"{0:.5f}\".format(recall(refsets['ynQuestion'], testsets['ynQuestion'])), \"\\t\\t\", \"{0:.5f}\".format(recall(refsets['NotQuestion'], testsets['NotQuestion'])))\n",
    "        return classifier\n",
    "    def DecisionTree(self, feature_set):\n",
    "        training_size = int(len(feature_set) * 0.1)\n",
    "        train_set, test_set = feature_set[training_size:], feature_set[:training_size]\n",
    "        classifier = nltk.classify.SklearnClassifier(DecisionTreeClassifier(max_depth=200)).train(train_set)\n",
    "       # print(\"accuracy = \",\"{0:.5f}\".format(nltk.classify.accuracy(classifier, test_set)))\n",
    "        refsets = collections.defaultdict(set)\n",
    "        testsets = collections.defaultdict(set)\n",
    "        for i, (feats, label) in enumerate(test_set):\n",
    "            refsets[label].add(i)\n",
    "            observed = classifier.classify(feats)\n",
    "            testsets[observed].add(i)\n",
    "#         print(\"\\t\\tWH-Question\\t\\tYN-Question\\t\\tNotQuestion\\n\")\n",
    "#         print( 'Precision:\\t', \"{0:.5f}\".format(precision(refsets['whQuestion'], testsets['whQuestion'])), \"\\t\\t\", \"{0:.5f}\".format(precision(refsets['ynQuestion'], testsets['ynQuestion'])), \"\\t\\t\", \"{0:.5f}\".format(precision(refsets['NotQuestion'], testsets['NotQuestion'])))\n",
    "#         print( 'Recall:   \\t', \"{0:.5f}\".format(recall(refsets['whQuestion'], testsets['whQuestion'])), \"\\t\\t\", \"{0:.5f}\".format(recall(refsets['ynQuestion'], testsets['ynQuestion'])), \"\\t\\t\", \"{0:.5f}\".format(recall(refsets['NotQuestion'], testsets['NotQuestion'])))\n",
    "        return classifier\n",
    "    def KNN(self, feature_set):\n",
    "        training_size = int(len(feature_set) * 0.1)\n",
    "        train_set, test_set = feature_set[training_size:], feature_set[:training_size]\n",
    "        classifier = nltk.classify.SklearnClassifier(KNeighborsClassifier()).train(train_set)\n",
    "        print(\"accuracy = \",\"{0:.5f}\".format(nltk.classify.accuracy(classifier, test_set)))\n",
    "        refsets = collections.defaultdict(set)\n",
    "        testsets = collections.defaultdict(set)\n",
    "        for i, (feats, label) in enumerate(test_set):\n",
    "            refsets[label].add(i)\n",
    "            observed = classifier.classify(feats)\n",
    "            testsets[observed].add(i)\n",
    "        print(\"\\t\\tWH-Question\\t\\tYN-Question\\t\\tNotQuestion\\n\")\n",
    "        print( 'Precision:\\t', \"{0:.5f}\".format(precision(refsets['whQuestion'], testsets['whQuestion'])), \"\\t\\t\", \"{0:.5f}\".format(precision(refsets['ynQuestion'], testsets['ynQuestion'])), \"\\t\\t\", \"{0:.5f}\".format(precision(refsets['NotQuestion'], testsets['NotQuestion'])))\n",
    "        print( 'Recall:   \\t', \"{0:.5f}\".format(recall(refsets['whQuestion'], testsets['whQuestion'])), \"\\t\\t\", \"{0:.5f}\".format(recall(refsets['ynQuestion'], testsets['ynQuestion'])), \"\\t\\t\", \"{0:.5f}\".format(recall(refsets['NotQuestion'], testsets['NotQuestion'])))\n",
    "        return classifier\n",
    "    def SVM(self, feature_set):\n",
    "        training_size = int(len(feature_set) * 0.1)\n",
    "        train_set, test_set = feature_set[training_size:], feature_set[:training_size]\n",
    "        classifier = nltk.classify.SklearnClassifier(LinearSVC()).train(train_set)\n",
    "        print(\"accuracy = \",\"{0:.5f}\".format(nltk.classify.accuracy(classifier, test_set)))\n",
    "        refsets = collections.defaultdict(set)\n",
    "        testsets = collections.defaultdict(set)\n",
    "        for i, (feats, label) in enumerate(test_set):\n",
    "            refsets[label].add(i)\n",
    "            observed = classifier.classify(feats)\n",
    "            testsets[observed].add(i)\n",
    "        print(\"\\t\\tWH-Question\\t\\tYN-Question\\t\\tNotQuestion\\n\")\n",
    "        print( 'Precision:\\t', \"{0:.5f}\".format(precision(refsets['whQuestion'], testsets['whQuestion'])), \"\\t\\t\", \"{0:.5f}\".format(precision(refsets['ynQuestion'], testsets['ynQuestion'])), \"\\t\\t\", \"{0:.5f}\".format(precision(refsets['NotQuestion'], testsets['NotQuestion'])))\n",
    "        print( 'Recall:   \\t', \"{0:.5f}\".format(recall(refsets['whQuestion'], testsets['whQuestion'])), \"\\t\\t\", \"{0:.5f}\".format(recall(refsets['ynQuestion'], testsets['ynQuestion'])), \"\\t\\t\", \"{0:.5f}\".format(recall(refsets['NotQuestion'], testsets['NotQuestion'])))\n",
    "        return classifier\n",
    "    def predict_question(self, text):\n",
    "        words = nltk.word_tokenize(text.lower())        \n",
    "        if '?' in text:\n",
    "            return 1\n",
    "        features = {}\n",
    "        for word in words:\n",
    "            features['contains({})'.format(word.lower())] = True            \n",
    "        prediction_result = self.classifier.classify(features)\n",
    "        if prediction_result == 'whQuestion' or prediction_result == 'ynQuestion':\n",
    "            return 1\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3731ef5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b62a9c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noOfSpeakers(speech_file):\n",
    "    from google.cloud import speech_v1p1beta1 as speech\n",
    "    client = speech.SpeechClient()\n",
    "    \n",
    "    gcs_uri=upload_to_bucket(\"meet_234\",speech_file,\"ml_jj1\")\n",
    "    \n",
    "    with open(speech_file, \"rb\") as audio_file:\n",
    "        content = audio_file.read()\n",
    "        \n",
    "    # The name of the audio file to transcribe\n",
    "    gcs_uri = \"gs://ml_jj1/meet_234\"\n",
    "    \n",
    "    audio = speech.RecognitionAudio(uri=gcs_uri)\n",
    "    diarization_config = speech.SpeakerDiarizationConfig(\n",
    "      enable_speaker_diarization=True,\n",
    "      min_speaker_count=2,\n",
    "      max_speaker_count=10,\n",
    "    )\n",
    "\n",
    "    config = speech.RecognitionConfig(\n",
    "        encoding = speech.RecognitionConfig.AudioEncoding.ENCODING_UNSPECIFIED,\n",
    "        sample_rate_hertz=8000,\n",
    "        language_code=\"en-US\",\n",
    "        diarization_config=diarization_config,\n",
    "    )\n",
    "    operation = client.long_running_recognize(config=config, audio=audio)\n",
    "    response = operation.result(timeout=500)\n",
    "    for result in response.results:\n",
    "        words_info = result.alternatives[0].words\n",
    "    speakers = []\n",
    "    noOfSpeakers = 0\n",
    "    for word_info in words_info:\n",
    "        speakers.append(int(word_info.speaker_tag))\n",
    "    speakers = set(speakers)\n",
    "    delete_blob(\"meet_234\",\"ml_jj1\")\n",
    "    return len(speakers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ba44b090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def silience(file_name):\n",
    "    from pydub import AudioSegment,silence\n",
    "    myaudio = intro = AudioSegment.from_wav(file_name)\n",
    "    silence = silence.detect_silence(myaudio, min_silence_len=1000, silence_thresh=-16)\n",
    "    silence = [((start/1000),(stop/1000)) for start,stop in silence] #convert to sec\n",
    "    return silence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4570f7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#return the list of questions\n",
    "def Question_detection(transcript):\n",
    "    \n",
    "    result = []\n",
    "    obj = isQuestion()\n",
    "    for i in transcript:\n",
    "        if(obj.predict_question(i) == 1):\n",
    "            result.append(i)\n",
    "    return result\n",
    "    #transcript is a list of setences ex. [\"hello everyone\",\"how is everyone\",\"lets start todays class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "05707883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_blob(blob_name, bucket_name):\n",
    "    \"\"\"Deletes a blob from the bucket.\"\"\"\n",
    "\n",
    "    storage_client = storage.Client.from_service_account_json(\n",
    "        'C:/Users/HP/Downloads/core-phoenix-341914-55f3e91fad73.json')\n",
    "\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    blob.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1bd2b8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "def upload_to_bucket(blob_name, path_to_file, bucket_name):\n",
    "    \"\"\" Upload data to a bucket\"\"\"\n",
    "\n",
    "    # Explicitly use service account credentials by specifying the private key\n",
    "    # file.\n",
    "    storage_client = storage.Client.from_service_account_json(\n",
    "        'C:/Users/HP/Downloads/core-phoenix-341914-55f3e91fad73.json')\n",
    "\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    blob.upload_from_filename(path_to_file)\n",
    "\n",
    "    #returns a public url\n",
    "    return blob.public_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b6811c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports the Google Cloud client library\n",
    "def speech_to_text(file_path):\n",
    "    \n",
    "    from google.cloud import speech\n",
    "\n",
    "    gcs_uri=upload_to_bucket(\"meet_234\",file_path,\"ml_jj1\")\n",
    "    # Instantiates a client\n",
    "    client = speech.SpeechClient()\n",
    "\n",
    "    # The name of the audio file to transcribe\n",
    "    gcs_uri = \"gs://ml_jj1/meet_234\"\n",
    "\n",
    "    audio = speech.RecognitionAudio(uri=gcs_uri)\n",
    "\n",
    "    config = speech.RecognitionConfig(\n",
    "        #encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "        encoding= speech.RecognitionConfig.AudioEncoding.ENCODING_UNSPECIFIED,\n",
    "        sample_rate_hertz=16000,\n",
    "        language_code=\"en-IN\",enable_word_time_offsets=True,enable_automatic_punctuation=True,\n",
    "    )\n",
    "\n",
    "    # Detects speech in the audio file\n",
    "    response = client.recognize(config=config, audio=audio)\n",
    "    word_stamps=[]\n",
    "    transcript=[]\n",
    "    \n",
    "    for result in response.results:\n",
    "        alternative = result.alternatives[0]\n",
    "        for word_info in alternative.words:\n",
    "            word = word_info.word\n",
    "            start_time = word_info.start_time\n",
    "            end_time = word_info.end_time\n",
    "            word_stamps.append([word,start_time,end_time])\n",
    "        transcript.append(alternative.transcript)\n",
    "    delete_blob(\"meet_234\",\"ml_jj1\")\n",
    "    return [transcript,word_stamps]\n",
    "   # return response.results \n",
    "#speech_to_text(\"test.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9309ae68",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words('english')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def my_tokenizer(doc):\n",
    "    words = word_tokenize(doc)\n",
    "    \n",
    "    pos_tags = pos_tag(words)\n",
    "    \n",
    "    non_stopwords = [w for w in pos_tags if not w[0].lower() in stopwords_list]\n",
    "    \n",
    "    non_punctuation = [w for w in non_stopwords if not w[0] in string.punctuation]\n",
    "    \n",
    "    lemmas = []\n",
    "    for w in non_punctuation:\n",
    "        if w[1].startswith('J'):\n",
    "            pos = wordnet.ADJ\n",
    "        elif w[1].startswith('V'):\n",
    "            pos = wordnet.VERB\n",
    "        elif w[1].startswith('N'):\n",
    "            pos = wordnet.NOUN\n",
    "        elif w[1].startswith('R'):\n",
    "            pos = wordnet.ADV\n",
    "        else:\n",
    "            pos = wordnet.NOUN\n",
    "        \n",
    "        lemmas.append(lemmatizer.lemmatize(w[0], pos))\n",
    "\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4610dd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question,all_data,k,threshold,tfidf_vectorizer,tfidf_matrix):\n",
    "    query_vect = tfidf_vectorizer.transform([question])\n",
    "    similarity = cosine_similarity(query_vect, tfidf_matrix)[0]\n",
    "    sim=[]\n",
    "    for i in range(len(similarity)):\n",
    "        sim.append((similarity[i],i))\n",
    "    sim.sort(reverse=True)\n",
    "    #use the top k results \n",
    "    interesting,difficult = 0,0\n",
    "    for max_indices in range(k):\n",
    "        max_index=sim[max_indices][1]\n",
    "        max_sim = sim[max_indices][0]\n",
    "        weight_diff = -1\n",
    "        weight_interesting = -1\n",
    "        if(all_data.iloc[max_index]['Is the question difficult?']=='Yes'): weight_diff=1\n",
    "        if(all_data.iloc[max_index]['Is the question interesting?']=='Yes'): weight_interesting=1\n",
    "        difficult+= weight_diff*max_sim\n",
    "        interesting+= weight_interesting*max_sim\n",
    "    \n",
    "    result={}\n",
    "    \n",
    "    if(difficult>=threshold):\n",
    "        result[\"difficulty\"]=\"Yes\"\n",
    "    else:\n",
    "        result[\"difficulty\"]=\"No\"\n",
    "    \n",
    "    if(interesting>=threshold):\n",
    "        result[\"interesting\"]=\"Yes\"\n",
    "    else:\n",
    "        result[\"interesting\"]=\"No\"\n",
    "        \n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c502ba9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interesting_questions(Questions,df,tfidf_vectorizer,tfidf_matrix):\n",
    "    diff,inter=0,0\n",
    "    diffl,interl=[],[]\n",
    "    for question in Questions:\n",
    "        diffl.append(0)\n",
    "        interl.append(0)\n",
    "        response = ask_question(question,df,1,0,tfidf_vectorizer,tfidf_matrix)\n",
    "        if(response[\"difficulty\"]==\"Yes\"): \n",
    "            diff+=1\n",
    "            diffl[-1]=1\n",
    "        if(response[\"interesting\"]==\"Yes\"): \n",
    "            inter+=1\n",
    "            interl[-1]=1\n",
    "    return [diff,inter,diffl,interl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1b9019ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_data():\n",
    "    #path to the form responses\n",
    "    df = pd.read_csv(\"C:/Users/HP/Desktop/TARP_J/question_feedback.csv\")\n",
    "    df.iloc[0:,1:]\n",
    "    df=df.rename(columns={'Write a question relevant to our subjects which profs ask(eg. What is NLP?, Can someone tell me the difference between ML and AI? etc.)':'Question'})\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer(tokenizer=my_tokenizer)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(tuple(df['Question']))\n",
    "    return [df,tfidf_matrix,tfidf_vectorizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "addfe7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def responseTime(transcript,word_stamps):\n",
    "    obj = isQuestion()\n",
    "    responseTimes=[]\n",
    "    words=0\n",
    "    total_time=0\n",
    "    for i in range(len(transcript)):\n",
    "        print(transcript[i],obj.predict_question(transcript[i]))\n",
    "        try:\n",
    "            if(obj.predict_question(transcript[i]) == 1):\n",
    "                j=i            \n",
    "                word_end_of_question=word_stamps[words+len(transcript[i].split(\" \"))-1][2]\n",
    "                passed=words\n",
    "                while(j<len(transcript) and obj.predict_question(transcript[j])==1):\n",
    "                    passed+=len(transcript[j].split(\" \"))\n",
    "                    j+=1\n",
    "                if(j<len(transcript) and passed<len(word_stamps)):\n",
    "                    if(passed+1<len(word_stamps)): passed+=1\n",
    "                    word_start_of_response=word_stamps[passed+1][1]\n",
    "                    responsetime = word_start_of_response-word_end_of_question\n",
    "                    responseTimes.append(math.ceil(responsetime.total_seconds()))\n",
    "            words+=len(transcript[i].split(\" \"))\n",
    "        except:\n",
    "            print(i)\n",
    "    return responseTimes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3b35fe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_transcript(transcript):\n",
    "    file1 = open('myfile.txt', 'w')\n",
    "    file1.writelines(transcript)\n",
    "    file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0aea222b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anvil.media\n",
    "from shutil import copy\n",
    "transcript,word_stamps=[],[]\n",
    "# @anvil.server.callable\n",
    "# def write_media_to_file(media_object):\n",
    "#     global transcript,word_stamps\n",
    "#     with anvil.media.TempFile(media_object) as f:\n",
    "#         #audio file stored in finally.mp3\n",
    "#         copy(f, \"finally.mp3\")\n",
    "#         #convert the speech to text\n",
    "#         transcript,word_stamps=speech_to_text(\"finally.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "437a2855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.94886\n",
      "\t\tWH-Question\t\tYN-Question\t\tNotQuestion\n",
      "\n",
      "Precision:\t 0.78689 \t\t 0.68056 \t\t 0.98050\n",
      "Recall:   \t 0.73846 \t\t 0.76562 \t\t 0.97627\n",
      "[]\n",
      "accuracy =  0.94886\n",
      "\t\tWH-Question\t\tYN-Question\t\tNotQuestion\n",
      "\n",
      "Precision:\t 0.78689 \t\t 0.68056 \t\t 0.98050\n",
      "Recall:   \t 0.73846 \t\t 0.76562 \t\t 0.97627\n",
      "matches any sankshep releasing is it something never worship from the civil war and vivid dreams about reservation system having public never come across any other across the state itself another word for very long and CPK 0\n"
     ]
    }
   ],
   "source": [
    "@anvil.server.callable\n",
    "def analyze():\n",
    "    global transcript\n",
    "    transcript,word_stamps=speech_to_text(\"finally.mp3\")\n",
    "    df,tfidf_matrix,tfidf_vectorizer=init_data()\n",
    "    Questions = Question_detection(transcript)\n",
    "    print(Questions)\n",
    "    Response_times = responseTime(transcript,word_stamps)\n",
    "    diff,inter,diffl,interl=interesting_questions(Questions,df,tfidf_vectorizer,tfidf_matrix)\n",
    "    speaker_count=noOfSpeakers(\"finally.mp3\")\n",
    "    return [5,3,1,1,2,[2,1,3],[0,0,1],[0,1,0]]\n",
    "    return [len(transcript),len(Questions),diff,inter,speaker_count,Response_times,diffl,interl]\n",
    "#analyze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a1c41147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.94886\n",
      "\t\tWH-Question\t\tYN-Question\t\tNotQuestion\n",
      "\n",
      "Precision:\t 0.78689 \t\t 0.68056 \t\t 0.98050\n",
      "Recall:   \t 0.73846 \t\t 0.76562 \t\t 0.97627\n",
      "[]\n",
      "accuracy =  0.94886\n",
      "\t\tWH-Question\t\tYN-Question\t\tNotQuestion\n",
      "\n",
      "Precision:\t 0.78689 \t\t 0.68056 \t\t 0.98050\n",
      "Recall:   \t 0.73846 \t\t 0.76562 \t\t 0.97627\n",
      "matches any sankshep releasing is it something never worship from the civil war and vivid dreams about reservation system having public never come across any other across the state itself another word for very long and CPK 0\n",
      "accuracy =  0.94886\n",
      "\t\tWH-Question\t\tYN-Question\t\tNotQuestion\n",
      "\n",
      "Precision:\t 0.78689 \t\t 0.68056 \t\t 0.98050\n",
      "Recall:   \t 0.73846 \t\t 0.76562 \t\t 0.97627\n",
      "[]\n",
      "accuracy =  0.94886\n",
      "\t\tWH-Question\t\tYN-Question\t\tNotQuestion\n",
      "\n",
      "Precision:\t 0.78689 \t\t 0.68056 \t\t 0.98050\n",
      "Recall:   \t 0.73846 \t\t 0.76562 \t\t 0.97627\n",
      "matches any sankshep releasing is it something never worship from the civil war and vivid dreams about reservation system having public never come across any other across the state itself another word for very long and CPK 0\n",
      "accuracy =  0.94886\n",
      "\t\tWH-Question\t\tYN-Question\t\tNotQuestion\n",
      "\n",
      "Precision:\t 0.78689 \t\t 0.68056 \t\t 0.98050\n",
      "Recall:   \t 0.73846 \t\t 0.76562 \t\t 0.97627\n",
      "[]\n",
      "accuracy =  0.94886\n",
      "\t\tWH-Question\t\tYN-Question\t\tNotQuestion\n",
      "\n",
      "Precision:\t 0.78689 \t\t 0.68056 \t\t 0.98050\n",
      "Recall:   \t 0.73846 \t\t 0.76562 \t\t 0.97627\n",
      "matches any sankshep releasing is it something never worship from the civil war and vivid dreams about reservation system having public never come across any other across the state itself another word for very long and CPK 0\n"
     ]
    }
   ],
   "source": [
    "@anvil.server.callable\n",
    "def SpeechToText():\n",
    "    global transcript\n",
    "    transcript=[\"Hello, can you tell me what is machine learning?\",\"machine learning is a branch of Artificial Intelligence and computer science which focuses on the use of data and algorithms to imitate the way humans learn\",\"Can you tell me what are the different frameworks which are available in JavaScript?\",\"the famous JavaScript frameworks are node.js view Angular JS and react.\",\"Could you tell me the founders of Google?\",\"the founders of Google are Larry Page and Sergey Brin\"]\n",
    "    return transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fac37995",
   "metadata": {},
   "outputs": [],
   "source": [
    "@anvil.server.callable\n",
    "def updateTranscript(Transcript):\n",
    "    global transcript\n",
    "    newTranscript=[]\n",
    "    for sentence in Transcript.split(\"\\n\"):\n",
    "        newTranscript.append(sentence)\n",
    "    transcript=newTranscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fefcdf1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.94886\n",
      "\t\tWH-Question\t\tYN-Question\t\tNotQuestion\n",
      "\n",
      "Precision:\t 0.78689 \t\t 0.68056 \t\t 0.98050\n",
      "Recall:   \t 0.73846 \t\t 0.76562 \t\t 0.97627\n",
      "[]\n",
      "accuracy =  0.94886\n",
      "\t\tWH-Question\t\tYN-Question\t\tNotQuestion\n",
      "\n",
      "Precision:\t 0.78689 \t\t 0.68056 \t\t 0.98050\n",
      "Recall:   \t 0.73846 \t\t 0.76562 \t\t 0.97627\n",
      "matches any sankshep releasing is it something never worship from the civil war and vivid dreams about reservation system having public never come across any other across the state itself another word for very long and CPK 0\n",
      "accuracy =  0.94886\n",
      "\t\tWH-Question\t\tYN-Question\t\tNotQuestion\n",
      "\n",
      "Precision:\t 0.78689 \t\t 0.68056 \t\t 0.98050\n",
      "Recall:   \t 0.73846 \t\t 0.76562 \t\t 0.97627\n",
      "[]\n",
      "accuracy =  0.94886\n",
      "\t\tWH-Question\t\tYN-Question\t\tNotQuestion\n",
      "\n",
      "Precision:\t 0.78689 \t\t 0.68056 \t\t 0.98050\n",
      "Recall:   \t 0.73846 \t\t 0.76562 \t\t 0.97627\n",
      "matches any sankshep releasing is it something never worship from the civil war and vivid dreams about reservation system having public never come across any other across the state itself another word for very long and CPK 0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3e8b00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25539281",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30b92cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd49087",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017b4056",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca0930a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a84339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2239ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
