{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b6bc7a3",
   "metadata": {},
   "source": [
    "## Installing and Importing the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e243a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#The line above is necesary to show Matplotlib's plots inside a Jupyter Notebook\n",
    "#!pip install opencv-contrib-python\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from PIL import Image\n",
    "from PIL import ImageFilter\n",
    "import cv2\n",
    "import numpy as np\n",
    "import anvil.server\n",
    "import anvil.server\n",
    "!pip install moviepy\n",
    "import moviepy.editor as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec709fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install azure-cognitiveservices-speech\n",
    "# !pip install google-api-python-client \n",
    "# !pip install --upgrade google-cloud-speech\n",
    "#!pip install anvil-uplink\n",
    "# !pip install --upgrade google-api-python-client\n",
    "# !pip install --upgrade google-cloud-speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1da9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anvil.server\n",
    "anvil.server.connect(\"server_PFOYW7TJUDIUZD4RDEWND7YY-ORZU3UXU6JCOS2XW\")\n",
    "import anvil.media\n",
    "from shutil import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d607d6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "# !pip install keras\n",
    "# !pip install tensorflow\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "from keras import models\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc57c42",
   "metadata": {},
   "source": [
    "## Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d90d1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('icml_face_data.csv')\n",
    "#48 x 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce92323",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef8f71d",
   "metadata": {},
   "source": [
    "## Functions for pre processing the data and for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbb29b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data):\n",
    "    \n",
    "    image_array = np.zeros(shape=(len(data), 48, 48))\n",
    "    image_label = np.array(list(map(int, data['emotion'])))\n",
    "    \n",
    "    for i, row in enumerate(data.index):\n",
    "        image = np.fromstring(data.loc[row, ' pixels'], dtype=int, sep=' ')\n",
    "        image = np.reshape(image, (48, 48))\n",
    "        image_array[i] = image\n",
    "        \n",
    "    return image_array, image_label\n",
    "\n",
    "def plot_examples(label=0):\n",
    "    fig, axs = plt.subplots(1, 5, figsize=(25, 12))\n",
    "    fig.subplots_adjust(hspace = .2, wspace=.2)\n",
    "    axs = axs.ravel()\n",
    "    for i in range(5):\n",
    "        idx = data[data['emotion']==label].index[i]\n",
    "        axs[i].imshow(train_images[idx][:,:,0], cmap='gray')\n",
    "        axs[i].set_title(emotions[train_labels[idx].argmax()])\n",
    "        axs[i].set_xticklabels([])\n",
    "        axs[i].set_yticklabels([])\n",
    "        \n",
    "def plot_all_emotions(train_images,emotions,train_labels):\n",
    "    fig, axs = plt.subplots(1, 7, figsize=(30, 12))\n",
    "    fig.subplots_adjust(hspace = .2, wspace=.2)\n",
    "    axs = axs.ravel()\n",
    "    for i in range(7):\n",
    "        idx = data[data['emotion']==i].index[i]\n",
    "        axs[i].imshow(train_images[idx][:,:,0], cmap='gray')\n",
    "        axs[i].set_title(emotions[train_labels[idx].argmax()])\n",
    "        axs[i].set_xticklabels([])\n",
    "        axs[i].set_yticklabels([])\n",
    "        \n",
    "def plot_image_and_emotion(emotions, test_image_array, test_image_label, pred_test_labels, image_number):\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 6), sharey=False)\n",
    "    \n",
    "    bar_label = emotions.values()\n",
    "    \n",
    "    axs[0].imshow(test_image_array[image_number], 'gray')\n",
    "    axs[0].set_title(emotions[test_image_label[image_number]])\n",
    "    \n",
    "    axs[1].bar(bar_label, pred_test_labels[image_number], color='orange', alpha=0.7)\n",
    "    axs[1].grid()\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def plot_compare_distributions(emotions, array1, array2, title1='', title2=''):\n",
    "    \n",
    "    df_array1 = pd.DataFrame()\n",
    "    df_array2 = pd.DataFrame()\n",
    "    df_array1['emotion'] = array1.argmax(axis=1)\n",
    "    df_array2['emotion'] = array2.argmax(axis=1)\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 6), sharey=False)\n",
    "    x = emotions.values()\n",
    "    \n",
    "    y = df_array1['emotion'].value_counts()\n",
    "    keys_missed = list(set(emotions.keys()).difference(set(y.keys())))\n",
    "    for key_missed in keys_missed:\n",
    "        y[key_missed] = 0\n",
    "    axs[0].bar(x, y.sort_index(), color='orange')\n",
    "    axs[0].set_title(title1)\n",
    "    axs[0].grid()\n",
    "    \n",
    "    y = df_array2['emotion'].value_counts()\n",
    "    keys_missed = list(set(emotions.keys()).difference(set(y.keys())))\n",
    "    for key_missed in keys_missed:\n",
    "        y[key_missed] = 0\n",
    "    axs[1].bar(x, y.sort_index())\n",
    "    axs[1].set_title(title2)\n",
    "    axs[1].grid()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e14012",
   "metadata": {},
   "source": [
    "## Driver function that drives the entire process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc77e4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def driverFunction(data):\n",
    "    data[' Usage'].value_counts()\n",
    "    emotions = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Sad', 5: 'Surprise', 6: 'Neutral'}\n",
    "    train_image_array, train_image_label = prepare_data(data[data[' Usage']=='Training'])\n",
    "    val_image_array, val_image_label = prepare_data(data[data[' Usage']=='PrivateTest'])\n",
    "    test_image_array, test_image_label = prepare_data(data[data[' Usage']=='PublicTest'])\n",
    "    train_images = train_image_array.reshape((train_image_array.shape[0], 48, 48, 1))\n",
    "    train_images = train_images.astype('float32')/255\n",
    "    val_images = val_image_array.reshape((val_image_array.shape[0], 48, 48, 1))\n",
    "    val_images = val_images.astype('float32')/255\n",
    "    test_images = test_image_array.reshape((test_image_array.shape[0], 48, 48, 1))\n",
    "    test_images = test_images.astype('float32')/255\n",
    "    train_labels = to_categorical(train_image_label)\n",
    "    val_labels = to_categorical(val_image_label)\n",
    "    test_labels = to_categorical(test_image_label)\n",
    "    plot_all_emotions(train_images,emotions,train_labels)\n",
    "    plot_compare_distributions(emotions,train_labels, val_labels, title1='train labels', title2='val labels')\n",
    "    class_weight = dict(zip(range(0, 7), (((data[data[' Usage']=='Training']['emotion'].value_counts()).sort_index())/len(data[data[' Usage']=='Training']['emotion'])).tolist()))\n",
    "    model = models.Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)))\n",
    "    model.add(MaxPool2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPool2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(7, activation='softmax'))\n",
    "    model.compile(optimizer=Adam(lr=1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    history = model.fit(train_images, train_labels,\n",
    "                    validation_data=(val_images, val_labels),\n",
    "                    class_weight = class_weight,\n",
    "                    epochs=12,\n",
    "                    batch_size=64)\n",
    "    test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "    print('test caccuracy:', test_acc)\n",
    "    pred_test_labels = model.predict(test_images)\n",
    "    plot_image_and_emotion(emotions,test_image_array, test_image_label, pred_test_labels, 104)\n",
    "    #return test_acc\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758a6009",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_array, test_image_label = prepare_data(data[data[' Usage']=='PublicTest'])\n",
    "test_images = test_image_array.reshape((test_image_array.shape[0], 48, 48, 1))\n",
    "test_images = test_images.astype('float32')/255\n",
    "pred_test_labels = model.predict(test_images[:1])\n",
    "print(pred_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76340803",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image_and_emotion(test_image_array, test_image_label, pred_test_labels, 104)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0216622b",
   "metadata": {},
   "source": [
    "## Functions for performing various Image Processing techniques "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7324f3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Grayscale(name1):\n",
    "    img = Image.open(name1)\n",
    "    imgGray = img.convert('L')\n",
    "    return np.array(imgGray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604e9939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Binary_Thresholding(name1,val,maxi):\n",
    "    image1 = cv2.imread(name1)\n",
    "    img = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "    ret, thresh1 = cv2.threshold(img,val,maxi, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    return np.array(thresh1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba757059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Truncated_Thresholding(name1,val,maxi):\n",
    "    image1 = cv2.imread(name1)\n",
    "    img = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "    ret, thresh2 = cv2.threshold(img,val,maxi,cv2.THRESH_TRUNC)\n",
    "    return np.array(thresh1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddc2ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Thresholding_To_Zero(name1,val = 120,maxi = 255):\n",
    "    image1 = cv2.imread(name1)\n",
    "    img = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "    ret, thresh3 = cv2.threshold(img,val,maxi,cv2.THRESH_TOZERO)\n",
    "    return np.array(thresh3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcc2e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert(name1):\n",
    "    image1 = cv2.imread(name1,0)\n",
    "    image1 = cv2.bitwise_not(image1)\n",
    "    return np.array(image1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1b1b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpen(name1):\n",
    "    img = Image.open(name1)\n",
    "    sharpened1 = img.filter(ImageFilter.SHARPEN);\n",
    "    sharpened2 = sharpened1.filter(ImageFilter.SHARPEN);\n",
    "    sharpened3 = sharpened2.filter(ImageFilter.SHARPEN);\n",
    "    return np.array(sharpened3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93b5a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce(array):\n",
    "    reduced=[]\n",
    "    for j in array:\n",
    "        row=[]\n",
    "        for i in j:\n",
    "            row.append(i[0])\n",
    "        reduced.append(row)\n",
    "    return np.array(reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cbecc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blur(name1):\n",
    "    img = cv2.imread(name1)\n",
    "    img = cv2.blur(img, (3,3)) \n",
    "    return reduce(np.array(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0156a76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min(name1):\n",
    "    img=Image.open(name1)\n",
    "    img2=img.filter(ImageFilter.MinFilter(size=3))\n",
    "    return np.array(img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4299909a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max(name1):\n",
    "    img=Image.open(name1)\n",
    "    img2=img.filter(ImageFilter.MaxFilter(size=3))\n",
    "    return np.array(img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815643b4",
   "metadata": {},
   "source": [
    "## Function for generating different combinations of IP techniques to be applied on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd34e11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d={0:\"Grey-scaling\",1:\"Thresholding\",2:\"Inversion\",3:\"Blurring\",4:\"Sharpening\",5:\"Min\",6:\"Max\"}\n",
    "#N=3 would trigger grey scaling and thresholding\n",
    "def GenerateCombinations(imageName, N): #give input as file name and mask, returns the new image pixel values\n",
    "     #number of different operations we have [greyscaling, thresholding etc\n",
    "    if(N==0):\n",
    "        return reduce(cv2.imread(imageName))\n",
    "    operationsList=[]\n",
    "    for j in range(0,32):\n",
    "        if(N&(1<<j)):\n",
    "            operationsList.append(d[j])\n",
    "    newArray=[]\n",
    "    print(operationsList)\n",
    "    for operations in operationsList:\n",
    "        enter=0\n",
    "        if(operations==\"Grey-scaling\"):\n",
    "            enter=1\n",
    "            newArray=Grayscale(imageName)\n",
    "        elif(operations==\"Thresholding\"):\n",
    "            enter=1\n",
    "            newArray=Binary_Thresholding(imageName, 100, 200)\n",
    "        elif(operations==\"Inversion\"):\n",
    "            enter=1\n",
    "            newArray=invert(imageName)\n",
    "        elif(operations == \"Blurring\"):\n",
    "            enter=1\n",
    "            newArray=blur(imageName)\n",
    "        elif(operations == \"Sharpening\"):\n",
    "            enter=1\n",
    "            newArray=sharpen(imageName)\n",
    "        elif(operations == \"Min\"):\n",
    "            enter=1\n",
    "            newArray=min(imageName)\n",
    "        elif(operations == \"Max\"):\n",
    "            enter=1\n",
    "            newArray=max(imageName)\n",
    "        if(enter):\n",
    "            new_image = Image.fromarray(newArray)\n",
    "            new_image.save(imageName)\n",
    "    return newArray\n",
    "\n",
    "#GenerateCombinations(\"anvil10_3.png\",32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3f6080",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60eb078",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask=3 #global mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906f1034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(image,prev):\n",
    "    count=0\n",
    "    for i in range(len(image)):\n",
    "        for j in range(len(image[i])):\n",
    "            for k in range(len(image[i][j])):\n",
    "                if(image[i][j][k]!=prev[i][j][k]):\n",
    "                    count+=1\n",
    "    if(count>230): #more than 10% change\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c6303e",
   "metadata": {},
   "source": [
    "## Extracting frames from video file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613b693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "emotions = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Sad', 5: 'Surprise', 6: 'Neutral'}\n",
    "freq = {'Angry': 0, 'Disgust': 0, 'Fear': 0, 'Happy': 0, 'Sad': 0, 'Surprise': 0, 'Neutral': 0}\n",
    "def getFrames(file):\n",
    "    global emotions\n",
    "    global freq\n",
    "    dataSet=\"Mask\"+str(mask)+\".csv\"\n",
    "    data = pd.read_csv(dataSet)\n",
    "    print(\"calling driver\")\n",
    "    model=driverFunction(data)\n",
    "    print(\"driver done\")\n",
    "    vidcap=cv2.VideoCapture(file) #capture and read the video file\n",
    "    success,image=vidcap.read()\n",
    "    prev=image\n",
    "    while success:\n",
    "        image=cv2.resize(image, (48,48), interpolation = cv2.INTER_AREA)\n",
    "        if(check(image,prev)):\n",
    "            cv2.imwrite(\"frame.png\",image)\n",
    "            image = cv2.imread(\"frame.png\")\n",
    "            #Show the image with matplotlib\n",
    "            plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "            plt.show()\n",
    "            newdata=\"\"\n",
    "            new_array=GenerateCombinations(\"frame.png\",mask) #place mask on the input image as well\n",
    "            plt.imshow(cv2.cvtColor(new_array, cv2.COLOR_BGR2RGB))\n",
    "            plt.show()\n",
    "            for i in new_array:\n",
    "                for j in i:\n",
    "                    newdata+=str(j)\n",
    "                    newdata+=\" \"\n",
    "            newdata=newdata[:-1]\n",
    "            row=[]\n",
    "            pix=[]\n",
    "            newdata=newdata.split(\" \")\n",
    "            for i in range(len(newdata)):\n",
    "                row.append([int(newdata[i])])\n",
    "                if(len(row)==48):\n",
    "                    pix.append(row)\n",
    "                    row=[]\n",
    "            array = np.array(pix, dtype=np.uint8)\n",
    "            test_images[0]=array #just change the first row of test_images \n",
    "            pred_test_labels = model.predict(test_images[:1])[0] #predict\n",
    "            mx=__builtin__.max(pred_test_labels)\n",
    "            for i in range(len(pred_test_labels)):\n",
    "                if(pred_test_labels[i]==mx):\n",
    "                    freq[emotions[i]]+=1\n",
    "        prev=image\n",
    "        success,image=vidcap.read()\n",
    "    return freq\n",
    "#print(getFrames(\"tr.mp4\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3378f3",
   "metadata": {},
   "source": [
    "## Extracting the audio from mp4 file and saving it as finally.mp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95f0414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAudio(file):\n",
    "    my_clip = mp.VideoFileClip(file)\n",
    "    my_clip.audio.write_audiofile(r\"finally.mp3\")\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804b9951",
   "metadata": {},
   "source": [
    "## Integration of backend with ANVIL frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adfa1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runApplication(fileName):\n",
    "    outputVideo=getFrames(fileName)\n",
    "    outputAudio=getAudio(fileName) #audio is written in finally.mp3 and sent to meetingOWL project [done]\n",
    "    return [outputVideo,outputAudio]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9f6935",
   "metadata": {},
   "outputs": [],
   "source": [
    "@anvil.server.callable\n",
    "def write_media_to_file(media_object):\n",
    "    with anvil.media.TempFile(media_object) as f:\n",
    "        #file stored in finally.mp3\n",
    "        copy(f, \"finally.mp4\")\n",
    "        #convert the speech to text\n",
    "        result=runApplication(\"finally.mp4\")\n",
    "        return result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39158a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@anvil.server.callable\n",
    "def analyze():\n",
    "    result=runApplication(\"finally.mp4\")\n",
    "    return result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5da5df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@anvil.server.callable\n",
    "def updateMask(bitNumber,status):\n",
    "    global mask\n",
    "    if(status==True):\n",
    "        mask|=(1<<bitNumber)\n",
    "    else:\n",
    "        mask&=~(1<<bitNumber)\n",
    "    print(mask)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297b0de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "emotions = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Sad', 5: 'Surprise', 6: 'Neutral'}\n",
    "freq = {'Angry': 0, 'Disgust': 0, 'Fear': 0, 'Happy': 0, 'Sad': 0, 'Surprise': 0, 'Neutral': 0}\n",
    "def getFrames(file,dataSet,model):\n",
    "    global emotions\n",
    "    global freq\n",
    "    print(\"calling driver\")\n",
    "    print(\"driver done\")\n",
    "    vidcap=cv2.VideoCapture(file) #capture and read the video file\n",
    "    success,image=vidcap.read()\n",
    "    prev=image\n",
    "    while success:\n",
    "        image=cv2.resize(image, (48,48), interpolation = cv2.INTER_AREA)\n",
    "        if(check(image,prev)):\n",
    "            cv2.imwrite(\"frame.png\",image)\n",
    "            image = cv2.imread(\"frame.png\")\n",
    "            newdata=\"\"\n",
    "            new_array=GenerateCombinations(\"frame.png\",mask) #place mask on the input image as well\n",
    "            plt.imshow(cv2.cvtColor(new_array, cv2.COLOR_BGR2RGB))\n",
    "            plt.show()\n",
    "            for i in new_array:\n",
    "                for j in i:\n",
    "                    newdata+=str(j)\n",
    "                    newdata+=\" \"\n",
    "            newdata=newdata[:-1]\n",
    "            row=[]\n",
    "            pix=[]\n",
    "            newdata=newdata.split(\" \")\n",
    "            for i in range(len(newdata)):\n",
    "                row.append([int(newdata[i])])\n",
    "                if(len(row)==48):\n",
    "                    pix.append(row)\n",
    "                    row=[]\n",
    "            array = np.array(pix, dtype=np.uint8)\n",
    "            test_images[0]=array #just change the first row of test_images \n",
    "            pred_test_labels = model.predict(test_images[:1])[0] #predict\n",
    "            mx=__builtin__.max(pred_test_labels)\n",
    "            for i in range(len(pred_test_labels)):\n",
    "                if(pred_test_labels[i]==mx):\n",
    "                    freq[emotions[i]]+=1\n",
    "        prev=image\n",
    "        success,image=vidcap.read()\n",
    "    return freq\n",
    "#print(getFrames(\"tr.mp4\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161d934c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Mask3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8868bccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=driverFunction(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c50032",
   "metadata": {},
   "outputs": [],
   "source": [
    "getFrames(\"interview.mp4\",data,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3721c684",
   "metadata": {},
   "outputs": [],
   "source": [
    "getAudio(\"interview.mp4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
